{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c64fb4a-592a-47bc-8d4a-f4e6a5a331a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from bert4torch.models import BaseModel\n",
    "from bert4torch.snippets import sequence_padding, Callback, ListDataset, EarlyStopping, get_pool_emb\n",
    "from bert4torch.callbacks import AdversarialTraining\n",
    "from bert4torch.optimizers import extend_with_exponential_moving_average, get_linear_schedule_with_warmup, Lion\n",
    "from bert4torch.tokenizers import Tokenizer\n",
    "from bert4torch.losses import MultilabelCategoricalCrossentropy\n",
    "from transformers import BertModel,AutoTokenizer\n",
    "from PromptModel import PromptTable\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e306224e-0d16-4cc0-b147-548907318154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "maxlen = 150\n",
    "batch_size = 16\n",
    "# 加载标签字典\n",
    "categories_label2id = {\"PER\": 1, \"BOOK\":2, \"OFI\": 3}\n",
    "categories_id2label = dict((value, key) for key, value in categories_label2id.items())\n",
    "ner_vocab_size = len(categories_label2id)\n",
    "ner_head_size = 64\n",
    "\n",
    "# BERT base\n",
    "config_path = './GujiBERT/config.json'\n",
    "checkpoint_path = './GujiBERT/pytorch_model.bin'\n",
    "dict_path = './GujiBERT/vocab.txt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75716b7e-8c14-4399-85e6-492bc840a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型结构\n",
    "class Model(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"./GujiBERT\")\n",
    "        self.prompttable = PromptTable(hidden_size=768,head_size=ner_head_size)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        sequence_output = self.bert(input_ids=token_ids, output_hidden_states=True)['last_hidden_state']\n",
    "        logit = self.prompttable(sequence_output, token_ids.gt(0).long())\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ab304b-c974-4a41-9137-87776eac13b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./GujiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./GujiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./GujiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./GujiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./GujiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./GujiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./GujiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./GujiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./GujiBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./GujiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#加载模型\n",
    "model1 = Model().to(device)\n",
    "model1.load_weights(f'./bestmodel/model1.pt')\n",
    "model2 = Model().to(device)\n",
    "model2.load_weights(f'./bestmodel/model2.pt')\n",
    "model3 = Model().to(device)\n",
    "model3.load_weights(f'./bestmodel/model3.pt')\n",
    "model4 = Model().to(device)\n",
    "model4.load_weights(f'./bestmodel/model4.pt')\n",
    "model5 = Model().to(device)\n",
    "model5.load_weights(f'./bestmodel/model5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad71fac0-d92d-48e8-a721-a897c6343903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_result(context, prediction):\n",
    "    textdict = defaultdict()\n",
    "    kdict =  defaultdict()\n",
    "    for i, char in enumerate(context):\n",
    "        textdict[i] = char\n",
    "        kdict[i] = False\n",
    "    entities = {}\n",
    "    for entity in prediction:\n",
    "        start = entity[0]\n",
    "        end = entity[1]\n",
    "        label = entity[2]\n",
    "        if kdict[start] == False and kdict[end]  == False:\n",
    "            textdict[start] = '{' + textdict[start]\n",
    "            textdict[end] = textdict[end] + '|'+label + '}'\n",
    "            kdict[start] = True\n",
    "            kdict[end] = True\n",
    "    t = ''\n",
    "    for k, v in textdict.items():\n",
    "        t += v\n",
    "    return t[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48e1a79-e22b-4eca-b04c-8038b901faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载测试集\n",
    "test = load_json('./GuNER2023_test_public.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1edc9daf-2b35-42a3-aff1-e0e9cc9d9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型一的预测结果\n",
    "labellist = []\n",
    "threshold = 0\n",
    "for text in test:\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids = torch.tensor([tokenizer.tokens_to_ids(tokens)], device=device).long()\n",
    "    output = model1.predict(token_ids)\n",
    "    output = output[0]\n",
    "    S = set()\n",
    "    for start, end in zip(*np.where(output.cpu() > threshold)):\n",
    "        if start <= end :\n",
    "            for rel in range(1, 4):\n",
    "                if output[start, rel] > threshold and output[rel, end] > threshold and output[end, start] > threshold and output[start, end] > threshold:\n",
    "                    S.add((mapping[start][0],mapping[end][-1], categories_id2label[rel]))\n",
    "    S = list(S)\n",
    "    labelset = defaultdict(int)\n",
    "    for entity in S:\n",
    "        labelset[entity] += 1\n",
    "    labellist.append(labelset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c1d5b39-3a88-405c-ae2b-7ad2e4a1c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型二的预测结果\n",
    "for i,text in enumerate(test,0):\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids = torch.tensor([tokenizer.tokens_to_ids(tokens)], device=device).long()\n",
    "    output = model2.predict(token_ids)\n",
    "    output = output[0]\n",
    "    S = set()\n",
    "    for start, end in zip(*np.where(output.cpu() > threshold)):\n",
    "        if start <= end :\n",
    "            for rel in range(1, 4):\n",
    "                if output[start, rel] > threshold and output[rel, end] > threshold and output[end, start] > threshold and output[start, end] > threshold:\n",
    "                    S.add((mapping[start][0],mapping[end][-1], categories_id2label[rel]))\n",
    "    S = list(S)\n",
    "    for entity in S:\n",
    "        labellist[i][entity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d34b7ecc-017d-4b37-bae1-5f907c7b845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型三的预测结果\n",
    "for i,text in enumerate(test,0):\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids = torch.tensor([tokenizer.tokens_to_ids(tokens)], device=device).long()\n",
    "    output = model3.predict(token_ids)\n",
    "    output = output[0]\n",
    "    S = set()\n",
    "    for start, end in zip(*np.where(output.cpu() > threshold)):\n",
    "        if start <= end :\n",
    "            for rel in range(1, 4):\n",
    "                if output[start, rel] > threshold and output[rel, end] > threshold and output[end, start] > threshold and output[start, end] > threshold:\n",
    "                    S.add((mapping[start][0],mapping[end][-1], categories_id2label[rel]))\n",
    "    S = list(S)\n",
    "    for entity in S:\n",
    "        labellist[i][entity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3166bd2-5415-4cb3-ae1e-15947911a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型四的预测结果\n",
    "for i,text in enumerate(test,0):\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids = torch.tensor([tokenizer.tokens_to_ids(tokens)], device=device).long()\n",
    "    output = model4.predict(token_ids)\n",
    "    output = output[0]\n",
    "    S = set()\n",
    "    for start, end in zip(*np.where(output.cpu() > threshold)):\n",
    "        if start <= end :\n",
    "            for rel in range(1, 4):\n",
    "                if output[start, rel] > threshold and output[rel, end] > threshold and output[end, start] > threshold and output[start, end] > threshold:\n",
    "                    S.add((mapping[start][0],mapping[end][-1], categories_id2label[rel]))\n",
    "    S = list(S)\n",
    "    for entity in S:\n",
    "        labellist[i][entity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "639d10e3-137a-497d-bdb9-5a80e7ac61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型五的预测结果\n",
    "for i,text in enumerate(test,0):\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids = torch.tensor([tokenizer.tokens_to_ids(tokens)], device=device).long()\n",
    "    output = model5.predict(token_ids)\n",
    "    output = output[0]\n",
    "    S = set()\n",
    "    for start, end in zip(*np.where(output.cpu() > threshold)):\n",
    "        if start <= end :\n",
    "            for rel in range(1, 4):\n",
    "                if output[start, rel] > threshold and output[rel, end] > threshold and output[end, start] > threshold and output[start, end] > threshold:\n",
    "                    S.add((mapping[start][0],mapping[end][-1], categories_id2label[rel]))\n",
    "    S = list(S)\n",
    "    for entity in S:\n",
    "        labellist[i][entity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1f33759-bc0c-483f-918d-dbb69e6f9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测结果融合\n",
    "La = []\n",
    "for labelset in labellist:\n",
    "    la = []\n",
    "    for label,value in labelset.items():\n",
    "        if value >= 3:\n",
    "            la.append(label)\n",
    "    La.append(la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6e6153a-bfbc-43f3-b7d7-227857295adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save2file(filename, prediction):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as fw:\n",
    "        for re in prediction:\n",
    "            fw.write(re + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b19028d-74bf-4845-bbff-41c652a56961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成结果文件\n",
    "test = load_json('./GuNER2023_test_public.txt')\n",
    "predictions = []\n",
    "for i,text in enumerate(test,0):\n",
    "    prediction = La[i]\n",
    "    R = package_result(text, prediction)\n",
    "    predictions.append(R)\n",
    "save2file('pred.txt', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecf282-8d08-414a-9133-386395dd83af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
